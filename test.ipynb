{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from typing import List\n",
    "\n",
    "import requests\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "# %matplotlib ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Leonard\\miniconda3\\envs\\openmmlab\\lib\\site-packages\\mmcv\\__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from local path: ../../Projects/mmdetection/checkpoints/mask_rcnn_swin-t-p4-w7_fpn_fp16_ms-crop-3x_coco_20210908_165006-90a4008c.pth\n"
     ]
    }
   ],
   "source": [
    "from mmdet.apis import init_detector, inference_detector, show_result_pyplot\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "config_file = '../../Projects/mmdetection/configs/swin/mask_rcnn_swin-t-p4-w7_fpn_fp16_ms-crop-3x_coco.py'\n",
    "checkpoint_file = '../../Projects/mmdetection/checkpoints/mask_rcnn_swin-t-p4-w7_fpn_fp16_ms-crop-3x_coco_20210908_165006-90a4008c.pth'\n",
    "\n",
    "# build the model from a config file and a checkpoint file\n",
    "model = init_detector(config_file, checkpoint_file, device='cuda:0')\n",
    "# model_ = init_detector(config_file, checkpoint_file, device='cuda:0')\n",
    "\n",
    "weights = ResNet18_Weights.IMAGENET1K_V1\n",
    "resnet = resnet18(weights=weights)\n",
    "resnet = nn.Sequential(*list(resnet.children())[:-1])\n",
    "resnet = resnet.eval().cuda()\n",
    "preprocess = weights.transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_dir = 'C:/Users/Leonard/AppData/LocalLow/DefaultCompany/Perception2/solo_16'\n",
    "# folder_dir = 'C:/Users/Leonard/AppData/LocalLow/DefaultCompany/Perception2/solo_6'\n",
    "# folder_dir = './data/solo/'\n",
    "\n",
    "errs = []\n",
    "preds, gts = [], []\n",
    "for i in range(20):\n",
    "    f_dir = f'{folder_dir}/sequence.{i}'\n",
    "    with open(f'{f_dir}/step0.frame_data.json') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    captures = data['captures']\n",
    "\n",
    "    \n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Line:\n",
    "    origin: np.ndarray\n",
    "    direction: np.ndarray\n",
    "\n",
    "    @classmethod\n",
    "    def from_camera_coor(cls, origin, coor):\n",
    "        return cls(np.array(origin), np.array(coor))\n",
    "    \n",
    "    def point(self, r):\n",
    "        return self.origin + r*self.direction\n",
    "\n",
    "\n",
    "def find_points(line_a: Line, line_b: Line):\n",
    "    d = line_b.origin - line_a.origin\n",
    "    u = np.dot(line_a.direction, line_b.direction)\n",
    "    e = np.dot(line_a.direction, d)\n",
    "    f = np.dot(line_b.direction, d)\n",
    "\n",
    "    r_1 = (e - u*f) / (1 - u**2)\n",
    "    r_2 = (f - u*e) / (u**2 - 1)\n",
    "\n",
    "    p1 = line_a.point(r_1)\n",
    "    p2 = line_b.point(r_2)\n",
    "\n",
    "    # print(p1, p2)\n",
    "    return (p1 + p2) / 2\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class Camera:\n",
    "    filename: str=''\n",
    "    position: np.ndarray=np.array([0., 0., 0.])\n",
    "    quaternion: np.ndarray=np.array([1., 0., 0., 0.])  # angle in quaternion\n",
    "\n",
    "    resolution: np.ndarray=np.array([3840, 2160])\n",
    "    sensor_size: np.ndarray=np.array([30, 30])\n",
    "    matrix: np.ndarray=np.array([0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "    # intrinsic: np.ndarray=np.array([[0, 0, 0], [0, 0, 0], [0, 0, 0]])\n",
    "    \n",
    "    @staticmethod\n",
    "    def qm_2(quaternion, vector):\n",
    "        q0, q1, q2, q3 = quaternion\n",
    "        matrix = np.array([\n",
    "            [1-2*q2**2-2*q3**2, 2*(q1*q2+q0*q3), 2*(q1*q3-q0*q2)],\n",
    "            [2*(q1*q2-q0*q3), 1-2*q1**2-2*q3**2, 2*(q2*q3+q0*q1)],\n",
    "            [2*(q1*q3+q0*q2), 2*(q2*q3-q0*q1), 1-2*q1**2-2*q2**2]\n",
    "        ])\n",
    "\n",
    "        return np.matmul(vector, matrix.T)\n",
    "\n",
    "    @staticmethod\n",
    "    def _convert_to_ndarray(obj):\n",
    "        if not isinstance(obj, (np.ndarray, np.generic)):\n",
    "            return np.array(obj)\n",
    "        return obj\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.position = self._convert_to_ndarray(self.position)\n",
    "        self.quaternion = self._convert_to_ndarray(self.quaternion)\n",
    "        self.resolution = self._convert_to_ndarray(self.resolution)\n",
    "        self.sensor_size = self._convert_to_ndarray(self.sensor_size)  # measured in millimeters\n",
    "        self.matrix = self._convert_to_ndarray(self.matrix).reshape((3, 3))\n",
    "\n",
    "        focal = 20.78461  # measured in millimeters\n",
    "        self.intrinsic = np.array([\n",
    "            [focal*self.resolution[0]/self.sensor_size[0], 0, 0],\n",
    "            [0, -focal*self.resolution[0]/self.sensor_size[1], 0],\n",
    "            [self.resolution[0]/2, self.resolution[1]/2, 1],\n",
    "        ])\n",
    "    \n",
    "    def world_to_camera(self, world_coor: List[np.ndarray]):\n",
    "        world_coor = [coor - self.position for coor in world_coor]  # translation\n",
    "        # rotation\n",
    "        camera_coor = [\n",
    "            self.qm_2(self.quaternion, coor)\n",
    "            for coor in world_coor\n",
    "        ]\n",
    "        return camera_coor\n",
    "\n",
    "    def camera_to_pixel(self, camera_coor: List[np.ndarray]) -> List[np.ndarray]:\n",
    "        ndc_coor = [np.matmul(coor, self.intrinsic) for coor in camera_coor]\n",
    "        pixel_coor = [np.floor((coor/coor[-1])[:-1]) for coor in ndc_coor]\n",
    "\n",
    "        return pixel_coor\n",
    "\n",
    "    def pixel_to_ray(self, pixel_coor):\n",
    "\n",
    "        inv_intrinsic = np.linalg.inv(self.intrinsic)\n",
    "        ndc_coor = [\n",
    "            np.matmul(np.array([*coor, 1]), inv_intrinsic)\n",
    "            for coor in pixel_coor\n",
    "        ]\n",
    "\n",
    "        q_ = self.quaternion\n",
    "        camera_coor = [\n",
    "            self.qm_2(np.array([q_[0], -q_[1], -q_[2], -q_[3]]), coor)\n",
    "            for coor in ndc_coor\n",
    "        ]\n",
    "\n",
    "        return [Line.from_camera_coor(self.position, coor) for coor in camera_coor]\n",
    "    \n",
    "    @classmethod\n",
    "    def from_capture(cls, f_dir, capture):\n",
    "        q_ = capture['rotation']\n",
    "        return cls(\n",
    "            filename=os.path.join(f_dir, capture['filename']),\n",
    "            position=capture['position'],\n",
    "            quaternion=np.array([q_[3], q_[0], q_[1], q_[2]]),\n",
    "            resolution=capture['dimension'],\n",
    "            matrix=capture['matrix'],\n",
    "        )\n",
    "    \n",
    "@dataclass\n",
    "class BoundingBox:\n",
    "    instanceId: str\n",
    "    center: np.ndarray\n",
    "    size: np.ndarray\n",
    "    \n",
    "    @classmethod\n",
    "    def from_anno(cls, info):\n",
    "        size = np.array([dim/2 for dim in info['dimension']])\n",
    "        return cls(\n",
    "            instanceId=info['instanceId'],\n",
    "            center=np.array(info['origin']) + size,\n",
    "            size=size\n",
    "        )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Location:\n",
    "    origin: np.ndarray\n",
    "\n",
    "    @classmethod\n",
    "    def from_info(cls, info, rotation, offset):\n",
    "        ret = Camera.qm_2(\n",
    "            rotation,\n",
    "            np.array(info['translation'])\n",
    "        )\n",
    "        return cls(ret + offset)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DeepCamera(Camera):\n",
    "    object_detector: torch.nn.Module=None\n",
    "    resnet: torch.nn.Module=None\n",
    "    preprocess: torch.nn.Module=None\n",
    "\n",
    "    gt_bboxes: List[BoundingBox]=field(default_factory=list)\n",
    "    location: Location=None\n",
    "\n",
    "    pred_bboxes: List[BoundingBox]=field(init=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        super().__post_init__()\n",
    "        result = inference_detector(self.object_detector, self.filename)\n",
    "        \n",
    "        for i in range(len(result[1])):\n",
    "            if len(result[1][i]) > 0:\n",
    "                for j in range(len(result[1][i])):\n",
    "                    result[1][i][j] = np.zeros_like(result[1][i][j], dtype=bool)\n",
    "        \n",
    "        image = Image.open(self.filename).convert('RGB')\n",
    "        human_pred = result[0][0]\n",
    "        self.preds = []\n",
    "        # for pred in human_pred:\n",
    "        for pred, pred_ in zip(human_pred, self.gt_bboxes):\n",
    "            if pred[-1] < 0.9:\n",
    "                continue\n",
    "            bbox = BoundingBox(\n",
    "                instanceId=-1,\n",
    "                center=np.array([(pred[0] + pred[2])/2, (pred[1] + pred[3])/2]),\n",
    "                size=np.array([(pred[2]-pred[0])/2, (pred[3]-pred[1])/2])\n",
    "            )\n",
    "            tl, br = pred_.center - pred_.size, pred_.center + pred_.size\n",
    "            image_ = image.crop((*tl, *br))\n",
    "            image_tensor = torch.tensor(np.array(image_), dtype=torch.float32).permute((2, 0, 1)).unsqueeze(0)\n",
    "            \n",
    "            image_tensor = self.preprocess(image_tensor)\n",
    "            with torch.no_grad():\n",
    "                box_feature = self.resnet(image_tensor.cuda())\n",
    "                print(box_feature.shape)\n",
    "            # print(box_feature[0][:20])\n",
    "\n",
    "            self.preds.append(\n",
    "                (bbox, pred[-1], box_feature[0].cpu(), self.pixel_to_ray([np.array([(pred[0] + pred[2])/2, (pred[1] + pred[3])/2])])[0])\n",
    "            )\n",
    "        print('-'*30)\n",
    "            \n",
    "    @classmethod\n",
    "    def from_capture(cls, f_dir, capture, object_detector, resnet, preprocess):\n",
    "        camera = Camera.from_capture(f_dir, capture)\n",
    "        annotations = [anno['values'] for anno in capture['annotations'] if '2D' in anno['id']][0]\n",
    "        # anno_3d = [(anno['instanceId'], Location.from_info(anno, rotation, offset)) for anno in anno_3d if obj_name in anno['labelName']]\n",
    "        return cls(\n",
    "            filename=camera.filename,\n",
    "            position=camera.position,\n",
    "            quaternion=camera.quaternion,\n",
    "            resolution=camera.resolution,\n",
    "            sensor_size=camera.sensor_size,\n",
    "            matrix=camera.matrix,\n",
    "            object_detector=object_detector,\n",
    "            resnet=resnet, preprocess=preprocess,\n",
    "            gt_bboxes=[BoundingBox.from_anno(anno) for anno in annotations]\n",
    "        )\n",
    "\n",
    "\n",
    "class Scene:\n",
    "    offset = np.array([0.00956252, 0, -0.068264])\n",
    "\n",
    "    def __init__(self, f_dir, cameras: List[DeepCamera], locations: List[Location]):\n",
    "        self.f_dir = f_dir\n",
    "        self.cameras = cameras\n",
    "        self.object_pairs = self._object_pairing()\n",
    "\n",
    "    def _object_pairing(self):\n",
    "        camera_0 = self.cameras[0].preds\n",
    "        camera_1 = self.cameras[1].preds\n",
    "\n",
    "        features_1 = torch.stack([pred[2] for pred in camera_1])\n",
    "        # print(features_1[:, :10])\n",
    "\n",
    "        for i, pred in enumerate(camera_0):\n",
    "            feature = pred[2]\n",
    "            # print(feature[:10])\n",
    "            similarity = F.cosine_similarity(feature.unsqueeze(0), features_1)\n",
    "            # print(similarity)\n",
    "            print(i, 'and', similarity.argmax(0))\n",
    "\n",
    "        for camera in self.cameras:\n",
    "            img = Image.open(camera.filename)\n",
    "            for pred in camera.preds:\n",
    "                bbox = pred[0]\n",
    "                tl, br = bbox.center - bbox.size, bbox.center + bbox.size\n",
    "                crop = img.crop((*tl, *br))\n",
    "                plt.imshow(crop)\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "\n",
    "            print('-'*30)\n",
    "\n",
    "        return None\n",
    "\n",
    "    @classmethod\n",
    "    def from_captures(cls, f_dir, captures, object_detector, resnet, preprocess):\n",
    "        cameras = [DeepCamera.from_capture(f_dir, capture, object_detector, resnet, preprocess) for capture in captures]\n",
    "        \n",
    "        return cls(\n",
    "            f_dir=f_dir,\n",
    "            cameras=cameras,\n",
    "            locations=[]\n",
    "        )\n",
    "    \n",
    "    def _show_bbox(self, bboxes):\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(mpimg.imread(self.filename))\n",
    "        for bbox in bboxes:\n",
    "            ax.add_patch(\n",
    "                patches.Rectangle(\n",
    "                    bbox.center - bbox.size,\n",
    "                    2*bbox.size[0], 2*bbox.size[1],\n",
    "                    linewidth=1, edgecolor='r', facecolor='none'\n",
    "                )\n",
    "            )\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    \n",
    "    def show_gt_bboxes(self):\n",
    "        for camera in self.cameras:\n",
    "            self._show_bbox(camera.gt_bboxes)\n",
    "    \n",
    "    def show_pred_bboxes(self):\n",
    "        for camera in self.cameras:\n",
    "            self._show_bbox([pred[0] for pred in camera.preds])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_dir = 'C:/Users/Leonard/AppData/LocalLow/DefaultCompany/Perception2/solo_16'\n",
    "# folder_dir = 'C:/Users/Leonard/AppData/LocalLow/DefaultCompany/Perception2/solo_6'\n",
    "# folder_dir = './data/solo/'\n",
    "\n",
    "errs = []\n",
    "preds, gts = [], []\n",
    "for i in range(20):\n",
    "    f_dir = f'{folder_dir}/sequence.{i}'\n",
    "    with open(f'{f_dir}/step0.frame_data.json') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    captures = data['captures']\n",
    "\n",
    "    scene = Scene.from_captures(f_dir, captures, model, resnet, preprocess)\n",
    "    # scene.show_gt_bboxes()\n",
    "    # scene.show_pred_bboxes()\n",
    "    # scene._object_pairing()\n",
    "\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    rays = []\n",
    "    colors = ['red', 'purple']\n",
    "    # fig = plt.figure(figsize=(8, 8))\n",
    "    # ax = fig.add_subplot(111, projection='3d')\n",
    "    for j, camera_info in enumerate(cameras):\n",
    "        # print('camera info', camera_info['position'], camera_info['rotation'])\n",
    "        q_ = camera_info['rotation']\n",
    "        camera = Camera(\n",
    "            position=camera_info['position'],\n",
    "            quaternion=np.array([q_[3], q_[0], q_[1], q_[2]]),\n",
    "            # quaternion=np.array([q_[0], q_[1], q_[2], q_[3]]),\n",
    "            resolution=camera_info['dimension'],\n",
    "            matrix=camera_info['matrix'],\n",
    "        )\n",
    "\n",
    "        # bbox_2d = _find(camera_info['annotations'], 'bounding box')\n",
    "        bbox_2d = _find(camera_info['annotations'], '2D')\n",
    "        center, size = bbox_2d['origin'], np.array([bbox/2 for bbox in bbox_2d['dimension']])\n",
    "        ray = camera.pixel_to_ray([center+size])\n",
    "        rays.append(ray[0])\n",
    "\n",
    "        # ax.quiver(\n",
    "        #     camera.position[0],\n",
    "        #     camera.position[1],\n",
    "        #     camera.position[2],\n",
    "        #     ray[0].direction[0],\n",
    "        #     ray[0].direction[1],\n",
    "        #     ray[0].direction[2],\n",
    "        #     color='teal', length=1.0, arrow_length_ratio=.1, normalize=True\n",
    "        # )\n",
    "        bbox_3d = _find(camera_info['annotations'], '3D')\n",
    "        # print(bbox_3d['translation'])\n",
    "\n",
    "        # bbox_3d = _find(camera_info['annotations'], 'bounding box 3D')\n",
    "        bbox_3d = _find(camera_info['annotations'], '3D')\n",
    "        # the bbox_3d['translation'] is the coordinate relative to the camera.\n",
    "        # Therefore, we need to convert it to world coordinate\n",
    "        # also, -0.2 in height as it is floating\n",
    "        q_ = camera.quaternion\n",
    "        ret = Camera.qm_2(\n",
    "            np.array([q_[0], -q_[1], -q_[2], -q_[3]]),\n",
    "            np.array(bbox_3d['translation'])\n",
    "        ) + camera.position + offset\n",
    "        # print(ret)\n",
    "        ret[1] = 1.01557275\n",
    "    # ret = np.array(bbox_3d['translation'])\n",
    "    # print('size', camera_info['size'])\n",
    "    # print(ret)\n",
    "    # get prediction\n",
    "    pred = find_points(*rays) #- np.array(offset)\n",
    "    err = np.sqrt(((pred - ret)**2).sum())\n",
    "    errs.append(err)\n",
    "    print(f'pred: {pred}, gt: {ret} {err:.4f}')\n",
    "    preds.append(pred)\n",
    "    gts.append(ret)\n",
    "\n",
    "    # # plot\n",
    "    # ax.scatter(*pred, color='red')\n",
    "    # ax.scatter(*ret, color='purple')\n",
    "    # plt.show()\n",
    "\n",
    "    # reproject the predicted location to each image\n",
    "    for j, camera_info in enumerate(cameras):\n",
    "        q_ = camera_info['rotation']\n",
    "        camera = Camera(\n",
    "            position=camera_info['position'],\n",
    "            quaternion=np.array([q_[3], q_[0], q_[1], q_[2]]),\n",
    "            resolution=camera_info['dimension'],\n",
    "            matrix=camera_info['matrix'],\n",
    "        )\n",
    "        # bbox_2d = _find(camera_info['annotations'], 'bounding box')\n",
    "        bbox_2d = _find(camera_info['annotations'], '2D')\n",
    "        center, size = bbox_2d['origin'], np.array([bbox/2 for bbox in bbox_2d['dimension']])\n",
    "        # center += size\n",
    "        camera_coor = camera.world_to_camera([ret])\n",
    "        pixel_coor = camera.camera_to_pixel(camera_coor)\n",
    "        print(center+size, pixel_coor[0], abs((center+size) - pixel_coor[0]).sum())\n",
    "\n",
    "    #     fig, ax = plt.subplots()\n",
    "    #     ax.imshow(mpimg.imread(f'{f_dir}/{camera_info[\"filename\"]}'))\n",
    "    #     ax.add_patch(\n",
    "    #         patches.Rectangle(\n",
    "    #             center,\n",
    "    #             2*size[0], 2*size[1],\n",
    "    #             linewidth=1, edgecolor='r', facecolor='none'\n",
    "    #         )\n",
    "    #     )\n",
    "    #     plt.axis('off')\n",
    "    #     plt.plot(\n",
    "    #         pixel_coor[0][0], pixel_coor[0][1],\n",
    "    #         \"o\", markersize=5, markeredgecolor=\"purple\", markerfacecolor=\"purple\"\n",
    "    #     )\n",
    "    #     plt.show()\n",
    "\n",
    "    print('-'*30)\n",
    "    break\n",
    "print(np.mean(errs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one = np.array([2.99043751, 1.01557281, -2.931736])\n",
    "two = np.array([-0.00956255, 1.01557269, 0.068264])\n",
    "\n",
    "one_gt = np.array([3, 0., -3])\n",
    "two_gt = np.array([0, 0., 0])\n",
    "size = bbox_3d['size']\n",
    "p = [one, two]\n",
    "gt = [one_gt, two_gt]\n",
    "\n",
    "offset = []\n",
    "for a, b in zip(p, gt):\n",
    "    offset.append(b-a)\n",
    "    print(b-a)\n",
    "    \n",
    "print('-'*30)\n",
    "print(np.var(offset, axis=0))\n",
    "offset = np.mean(offset, axis=0)\n",
    "print(offset)\n",
    "print('-'*30)\n",
    "for a, b in zip(p, gt):\n",
    "    print(\n",
    "        np.array(b) - np.array(offset) - np.array(a)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = bbox_3d['size']\n",
    "errs = []\n",
    "bins = []\n",
    "for pred, gt in zip(preds, gts):\n",
    "    err = np.sqrt((pred - gt)**2)\n",
    "    print(err)\n",
    "    errs.append(err)\n",
    "    bins.append(err < size)\n",
    "print('-'*30)\n",
    "print(np.sum(bins, axis=0)/len(bins))\n",
    "print(np.mean(errs, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find(l, s):\n",
    "    for elem in l:\n",
    "        if elem['id'] == s or elem['id'] == f'{s}_0':\n",
    "            values = elem['values']\n",
    "            for i, value in enumerate(values):\n",
    "                if value['labelName'] == 'Person':\n",
    "                # if value['labelName'] == 'cleaning_snuggle_henkel':\n",
    "                    break\n",
    "            else:\n",
    "                return None\n",
    "            return values[i]\n",
    "\n",
    "\n",
    "folder_dir = 'C:/Users/Leonard/AppData/LocalLow/DefaultCompany/Perception2/solo_11'\n",
    "\n",
    "errs = []\n",
    "preds, gts = [], []\n",
    "for i in range(20):\n",
    "    f_dir = f'{folder_dir}/sequence.{i}'\n",
    "    with open(f'{f_dir}/step0.frame_data.json') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    cameras = data['captures']\n",
    "\n",
    "    rays = []\n",
    "    colors = ['red', 'purple']\n",
    "    for j, camera_info in enumerate(cameras):\n",
    "        # print('camera info', camera_info['position'], camera_info['rotation'])\n",
    "        q_ = camera_info['rotation']\n",
    "        camera = Camera(\n",
    "            position=camera_info['position'],\n",
    "            quaternion=np.array([q_[3], q_[0], q_[1], q_[2]]),\n",
    "            resolution=camera_info['dimension'],\n",
    "            matrix=camera_info['matrix'],\n",
    "        )\n",
    "\n",
    "    # bbox_3d = _find(camera_info['annotations'], 'bounding box 3D')\n",
    "    bbox_3d = _find(camera_info['annotations'], '3D')\n",
    "    q_ = camera.quaternion\n",
    "    ret = Camera.qm_2(\n",
    "        np.array([q_[0], -q_[1], -q_[2], -q_[3]]),\n",
    "        np.array(bbox_3d['translation'])\n",
    "    ) + camera.position - np.array([0, 0.1, 0])\n",
    "\n",
    "    # reproject the predicted location to each image\n",
    "    for j, camera_info in enumerate(cameras):\n",
    "        q_ = camera_info['rotation']\n",
    "        camera = Camera(\n",
    "            position=camera_info['position'],\n",
    "            quaternion=np.array([q_[3], q_[0], q_[1], q_[2]]),\n",
    "            resolution=camera_info['dimension'],\n",
    "            matrix=camera_info['matrix'],\n",
    "        )\n",
    "        # bbox_2d = _find(camera_info['annotations'], 'bounding box')\n",
    "        bbox_2d = _find(camera_info['annotations'], '2D')\n",
    "        center, size = bbox_2d['origin'], np.array([bbox/2 for bbox in bbox_2d['dimension']])\n",
    "        # center += size\n",
    "        camera_coor = camera.world_to_camera([ret])\n",
    "        pixel_coor = camera.camera_to_pixel(camera_coor)\n",
    "        print(\n",
    "            center+size,\n",
    "            pixel_coor[0],\n",
    "            abs((center+size) - pixel_coor[0]).sum()\n",
    "        )\n",
    "\n",
    "    print('-'*30)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_3d['translation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16 (default, Mar  2 2023, 03:18:16) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b09d746b8f7d49e4d7c870418bd04bb5a79717e9e3cc6f92d77e11e9d41b7621"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
