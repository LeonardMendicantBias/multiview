{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from typing import List\n",
    "# from base import Scene\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from pytorch_metric_learning.losses import ContrastiveLoss, LiftedStructureLoss\n",
    "from pytorch_metric_learning.distances import CosineSimilarity\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "# %matplotlib ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmdet.apis import init_detector, inference_detector, show_result_pyplot\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision.models import resnet101, ResNet101_Weights\n",
    "\n",
    "config_file = '../../Projects/mmdetection/configs/swin/mask_rcnn_swin-t-p4-w7_fpn_fp16_ms-crop-3x_coco.py'\n",
    "checkpoint_file = '../../Projects/mmdetection/checkpoints/mask_rcnn_swin-t-p4-w7_fpn_fp16_ms-crop-3x_coco_20210908_165006-90a4008c.pth'\n",
    "\n",
    "# build the model from a config file and a checkpoint file\n",
    "model = init_detector(config_file, checkpoint_file, device='cuda:0')\n",
    "\n",
    "folder_dir = 'C:/Users/Leonard/AppData/LocalLow/DefaultCompany/Perception2/solo_17'\n",
    "training_dir = 'C:/Users/Leonard/data'\n",
    "testing_dir = 'C:/Users/Leonard/data_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(100):\n",
    "    print(f'\\r{i}', end='')\n",
    "    f_dir = f'{folder_dir}/sequence.{i}'\n",
    "    with open(f'{f_dir}/step0.frame_data.json') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    captures = data['captures']\n",
    "\n",
    "    for j, capture in enumerate(captures):\n",
    "        filename = f'{f_dir}/{capture[\"filename\"]}'\n",
    "        image = Image.open(filename).convert('RGB')\n",
    "        annotations = capture['annotations']\n",
    "        bboxes = [anno['values'] for anno in annotations if \"2D\" in anno['id']][0]\n",
    "        \n",
    "        for z, bbox in enumerate(bboxes):\n",
    "            size = np.array([dim/2 for dim in bbox['dimension']])\n",
    "            center = np.array(bbox['origin']) + size\n",
    "\n",
    "            tl, br = center - size, center + size\n",
    "            crop = image.crop((*tl, *br))\n",
    "            crop.save(f'{training_dir}/{bbox[\"labelName\"]}/{i}_{j}.png')\n",
    "            # plt.title(f'{bbox[\"labelName\"]}')\n",
    "            # plt.show()\n",
    "\n",
    "        # break\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = ResNet50_Weights.IMAGENET1K_V2\n",
    "resnet = resnet50(weights=weights)\n",
    "# weights = ResNet101_Weights.IMAGENET1K_V2\n",
    "# resnet = resnet101(weights=weights)\n",
    "resnet = nn.Sequential(*list(resnet.children())[:-1])\n",
    "resnet = resnet.cuda()\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "train_ds = torchvision.datasets.ImageFolder(training_dir, transform=preprocess)\n",
    "train_loader = DataLoader(train_ds, batch_size=32, pin_memory=True, num_workers=0, shuffle=True)\n",
    "\n",
    "torch.manual_seed(222)\n",
    "distance = CosineSimilarity()\n",
    "# loss_fnc = ContrastiveLoss(pos_margin=0.9, neg_margin=0.1, distance=distance)\n",
    "# loss_fnc = ContrastiveLoss(pos_margin=0.1, neg_margin=0.9)\n",
    "# loss_fnc = LiftedStructureLoss(pos_margin=0.9, neg_margin=0.1, distance=distance)\n",
    "loss_fnc = LiftedStructureLoss(pos_margin=0.1, neg_margin=0.9)\n",
    "optimizer = torch.optim.Adam(resnet.parameters(), lr=5e-5)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "step = 0\n",
    "for epoch in range(3):\n",
    "    losses = []\n",
    "    for images, labels in train_loader:\n",
    "        step += 1\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            features = resnet(images.cuda())[:, :, 0, 0]\n",
    "            loss = loss_fnc(features, labels)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        print(f'\\r{epoch} {step} {np.mean(losses)}', end='')\n",
    "        # if step == 100:\n",
    "        #     break\n",
    "    print()\n",
    "\n",
    "    # for i in range(100, 150):\n",
    "    # print(f'\\r{i}', end='')\n",
    "    f_dir = f'{folder_dir}/sequence.100'\n",
    "    with open(f'{f_dir}/step0.frame_data.json') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    captures = data['captures']\n",
    "\n",
    "    instances = {}\n",
    "\n",
    "    for j, capture in enumerate(captures):\n",
    "        filename = f'{f_dir}/{capture[\"filename\"]}'\n",
    "        image = Image.open(filename).convert('RGB')\n",
    "        annotations = capture['annotations']\n",
    "        result = inference_detector(model, filename)\n",
    "        human_pred = [ret for ret in result[0][0] if ret[-1] > 0.9]\n",
    "        \n",
    "        for pred in human_pred:\n",
    "            center=np.array([(pred[0] + pred[2])/2, (pred[1] + pred[3])/2])\n",
    "            size=np.array([(pred[2]-pred[0])/2, (pred[3]-pred[1])/2])\n",
    "            \n",
    "        # bboxes = [anno['values'] for anno in annotations if \"2D\" in anno['id']][0]\n",
    "        \n",
    "        # for z, bbox in enumerate(bboxes):\n",
    "        #     size = np.array([dim/2 for dim in bbox['dimension']])\n",
    "        #     center = np.array(bbox['origin']) + size\n",
    "\n",
    "            tl, br = center - size, center + size\n",
    "            crop = image.crop((*tl, *br))\n",
    "            with torch.no_grad():\n",
    "                feature = resnet(preprocess(crop).unsqueeze(0).cuda()).cpu()[0, :, 0, 0]\n",
    "            obj = {\n",
    "                'instanceId': None,\n",
    "                'image': crop,\n",
    "                'features': feature\n",
    "            }\n",
    "            if instances.get(capture[\"id\"], None) is None:\n",
    "                instances[capture[\"id\"]] = [obj]\n",
    "            else:\n",
    "                instances[capture[\"id\"]].append(obj)\n",
    "\n",
    "    instances_0 = instances[\"camera_0\"]\n",
    "    instances_1 = instances[\"camera\"]\n",
    "\n",
    "    features_1 = torch.stack([instance[\"features\"] for instance in instances_1], dim=0)\n",
    "\n",
    "    distance = CosineSimilarity()\n",
    "    ds = []\n",
    "    for instance in instances_0:\n",
    "        f = instance[\"features\"].unsqueeze(0)\n",
    "\n",
    "        d = distance(f, features_1)\n",
    "        ds.append(d)\n",
    "        # print(d)\n",
    "    print(abs(ds[1] - ds[0]))\n",
    "        # break\n",
    "    # break\n",
    "    # break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor([[0.1167, 0.0438]])  \n",
    "tensor([[0.2519, 0.1248]])  \n",
    "tensor([[0.3222, 0.1715]])  \n",
    "tensor([[0.3459, 0.2030]])  \n",
    "tensor([[0.3637, 0.2037]])  \n",
    "tensor([[0.3626, 0.1916]])  \n",
    "tensor([[0.3517, 0.1961]])  \n",
    "tensor([[0.3379, 0.1876]])  \n",
    "tensor([[0.3295, 0.1583]])  \n",
    "tensor([[0.3229, 0.1450]])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "222-3-.028, 0.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(resnet.state_dict(), './best.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = ResNet50_Weights.IMAGENET1K_V2\n",
    "resnet = resnet50(weights=weights)\n",
    "\n",
    "resnet = nn.Sequential(*list(resnet.children())[:-1])\n",
    "resnet = resnet.cuda()\n",
    "preprocess = weights.transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_dir = 'C:/Users/Leonard/AppData/LocalLow/DefaultCompany/Perception2/solo_17'\n",
    "testing_dir = 'C:/Users/Leonard/data_test'\n",
    "\n",
    "distance = CosineSimilarity()\n",
    "\n",
    "acc = 0\n",
    "count = 0\n",
    "n_obj = 0\n",
    "\n",
    "resnet.eval()\n",
    "# for i in range(100, 110):\n",
    "for i in range(100, 150):\n",
    "    # print(f'\\r{i}', end='')\n",
    "    f_dir = f'{folder_dir}/sequence.{i}'\n",
    "    with open(f'{f_dir}/step0.frame_data.json') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    captures = data['captures']\n",
    "\n",
    "    bbox_dict = {}\n",
    "    det_obj = 100\n",
    "    for j, capture in enumerate(captures):\n",
    "        filename = f'{f_dir}/{capture[\"filename\"]}'\n",
    "        image = Image.open(filename).convert('RGB')\n",
    "        camera_id = capture['id']\n",
    "\n",
    "        # gt bounding box and instance Id\n",
    "        annotations = capture['annotations']\n",
    "        bboxes = [anno['values'] for anno in annotations if \"2D\" in anno['id']][0]\n",
    "        for z, bbox in enumerate(bboxes):\n",
    "            if bbox_dict.get(bbox['instanceId'], None) is None:\n",
    "                bbox_dict[bbox['instanceId']] = {}\n",
    "                \n",
    "            # print(bbox)\n",
    "            size = np.array([dim/2 for dim in bbox['dimension']])\n",
    "            center = np.array(bbox['origin']) + size\n",
    "            tl, br = center - size, center + size\n",
    "            crop = image.crop((*tl, *br))\n",
    "            obj = {}\n",
    "            with torch.no_grad():\n",
    "                feature_temp = resnet(preprocess(crop).unsqueeze(0).cuda()).cpu()[0, :, 0, 0]\n",
    "            obj['gt'] = {\n",
    "                'center': center,\n",
    "                'size': size,\n",
    "                'features': feature_temp,\n",
    "                'image': crop,\n",
    "            }\n",
    "            bbox_dict[bbox['instanceId']][camera_id] = obj\n",
    "\n",
    "        # pred\n",
    "        result = inference_detector(model, filename)\n",
    "        human_pred = [ret for ret in result[0][0] if ret[-1] > 0.9]\n",
    "        # print(human_pred)\n",
    "        det_obj = min(len(human_pred), det_obj)\n",
    "        \n",
    "        crops = {}\n",
    "        for pred in human_pred:\n",
    "            center = np.array([(pred[0] + pred[2])/2, (pred[1] + pred[3])/2])\n",
    "            size = np.array([(pred[2]-pred[0])/2, (pred[3]-pred[1])/2])\n",
    "            tl, br = center - size, center + size\n",
    "            crop = image.crop((*tl, *br))\n",
    "\n",
    "            t, l = tl\n",
    "            b, r = br\n",
    "            \n",
    "            max_iou = 0\n",
    "            key_idx = None\n",
    "            for key in bbox_dict:\n",
    "                bbox_d = bbox_dict[key][camera_id]['gt']\n",
    "                tl_, br_ = bbox_d['center'] - bbox_d['size'], bbox_d['center'] + bbox_d['size']\n",
    "                t_, l_ = tl_\n",
    "                b_, r_ = br_\n",
    "\n",
    "                o = np.array([max(t, t_), max(l, l_)])\n",
    "                u = np.array([min(b, b_), min(r, r_)])\n",
    "\n",
    "                w = u[0] - o[0]\n",
    "                h = u[1] - o[1]\n",
    "                intersection = w * h\n",
    "                union = ((b-t)*(r-l)) + ((b_-t_)*(r_-l_)) - intersection\n",
    "                iou = intersection / (union + 1e-8)\n",
    "                if iou > max_iou:\n",
    "                    key_idx = key\n",
    "                    max_iou = iou\n",
    "                    \n",
    "            with torch.no_grad():\n",
    "                feature_temp = resnet(preprocess(crop).unsqueeze(0).cuda()).cpu()[0, :, 0, 0]\n",
    "            bbox_d = bbox_dict[key_idx][camera_id]['pred'] = {\n",
    "                'center': center,\n",
    "                'size': size,\n",
    "                'features': feature_temp,\n",
    "                # 'image': crop\n",
    "            }\n",
    "    n_obj += det_obj\n",
    "\n",
    "    # for instance_key in bbox_dict:\n",
    "    f_1_0 = bbox_dict[1]['camera_0'].get('pred', {'features': torch.zeros(2048)})['features']\n",
    "    f_1_1 = bbox_dict[1]['camera'].get('pred', {'features': torch.zeros(2048)})['features']\n",
    "\n",
    "    f_2_0 = bbox_dict[2]['camera_0'].get('pred', {'features': torch.zeros(2048)})['features']\n",
    "    f_2_1 = bbox_dict[2]['camera'].get('pred', {'features': torch.zeros(2048)})['features']\n",
    "\n",
    "    distance = CosineSimilarity()\n",
    "\n",
    "    fs = torch.stack([f_1_1, f_2_1], dim=0)\n",
    "    s_12 = distance(f_1_0.unsqueeze(0), fs)\n",
    "    s_22 = distance(f_2_0.unsqueeze(0), fs)\n",
    "\n",
    "    a, b = torch.concat([s_12, s_22], dim=0).argmax(0).tolist()\n",
    "    print(i, a, b)\n",
    "    if a == 0 and not f_1_0.eq(zero).all(): acc += 1\n",
    "    if b == 1 and not f_2_0.eq(zero).all(): acc += 1\n",
    "\n",
    "    zero = torch.zeros(2048)\n",
    "    if not (f_1_0.eq(zero).all() or f_2_0.eq(zero).all()):\n",
    "        count += 1\n",
    "    if not (f_1_1.eq(zero).all() or f_2_1.eq(zero).all()):\n",
    "        count += 1\n",
    "\n",
    "    # break\n",
    "# print(bbox_dict[1])\n",
    "# print(bbox_dict[2])\n",
    "\n",
    "print()\n",
    "print(acc)\n",
    "print(count, n_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "52/91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_dict[1]['camera']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for instance_key in bbox_dict:\n",
    "f_1_0 = bbox_dict[1]['camera_0']['pred']['features']\n",
    "f_1_1 = bbox_dict[1]['camera']['pred']['features']\n",
    "\n",
    "f_2_0 = bbox_dict[2]['camera_0']['pred']['features']\n",
    "f_2_1 = bbox_dict[2]['camera']['pred']['features']\n",
    "\n",
    "distance = CosineSimilarity()\n",
    "\n",
    "fs = torch.stack([f_1_1, f_2_1], dim=0)\n",
    "s_12 = distance(f_1_0.unsqueeze(0), fs)\n",
    "s_22 = distance(f_2_0.unsqueeze(0), fs)\n",
    "\n",
    "print(s_12)\n",
    "print(s_22)\n",
    "print('-'*30)\n",
    "\n",
    "print(\n",
    "    torch.concat([s_12, s_22], dim=0)\n",
    ")\n",
    "i, j = torch.concat([s_12, s_22], dim=0).argmax(0).tolist()\n",
    "print(i+1, j+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet(preprocess(crop).unsqueeze(0).cuda()).cpu().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    # max_iou = 0\n",
    "    # for j, capture in enumerate(captures):\n",
    "    #     tl_, br_ = center - size, center + size\n",
    "    #     t_, l_ = tl_\n",
    "    #     b_, r_ = br_\n",
    "\n",
    "    #     o = np.array([max(t, t_), max(l, l_)])\n",
    "    #     u = np.array([min(b, b_), min(r, r_)])\n",
    "\n",
    "    #     w = u[0] - o[0]\n",
    "    #     h = u[1] - o[1]\n",
    "    #     intersection = w * h\n",
    "    #     union = ((b-t)*(r-l)) + ((b_-t_)*(r_-l_)) - intersection\n",
    "    #     iou = intersection / (union + 1e-8)\n",
    "    #     max_iou = max(max_iou, iou)\n",
    "\n",
    "    # instances = {}\n",
    "\n",
    "    # for j, capture in enumerate(captures):\n",
    "    #     filename = f'{f_dir}/{capture[\"filename\"]}'\n",
    "    #     image = Image.open(filename).convert('RGB')\n",
    "    #     annotations = capture['annotations']\n",
    "    #     result = inference_detector(model, filename)\n",
    "    #     human_pred = [ret for ret in result[0][0] if ret[-1] > 0.9]\n",
    "        \n",
    "    #     for pred in human_pred:\n",
    "    #         center=np.array([(pred[0] + pred[2])/2, (pred[1] + pred[3])/2])\n",
    "    #         size=np.array([(pred[2]-pred[0])/2, (pred[3]-pred[1])/2])\n",
    "            \n",
    "    #     # bboxes = [anno['values'] for anno in annotations if \"2D\" in anno['id']][0]\n",
    "        \n",
    "    #     # for z, bbox in enumerate(bboxes):\n",
    "    #     #     size = np.array([dim/2 for dim in bbox['dimension']])\n",
    "    #     #     center = np.array(bbox['origin']) + size\n",
    "\n",
    "    #         tl, br = center - size, center + size\n",
    "    #         crop = image.crop((*tl, *br))\n",
    "            \n",
    "    #         with torch.no_grad():\n",
    "    #             feature = resnet(preprocess(crop).unsqueeze(0).cuda()).cpu()[0, :, 0, 0]\n",
    "    #         feature_list.append(feature)\n",
    "    #         obj = {\n",
    "    #             'instanceId': None,\n",
    "    #             'image': crop,\n",
    "    #             'features': feature\n",
    "    #         }\n",
    "    #         if instances.get(capture[\"id\"], None) is None:\n",
    "    #             instances[capture[\"id\"]] = [obj]\n",
    "    #         else:\n",
    "    #             instances[capture[\"id\"]].append(obj)\n",
    "\n",
    "    # instances_0 = instances[\"camera_0\"]\n",
    "    # instances_1 = instances[\"camera\"]\n",
    "\n",
    "    # features_1 = torch.stack([instance[\"features\"] for instance in instances_1], dim=0)\n",
    "\n",
    "    # distance = CosineSimilarity()\n",
    "    # for instance in instances_0:\n",
    "    #     f = instance[\"features\"].unsqueeze(0)\n",
    "\n",
    "    #     d = distance(f, features_1)\n",
    "    #     print(d)\n",
    "\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_names = ['camera_0', 'camera']\n",
    "\n",
    "instances_0 = instances[\"camera_0\"]\n",
    "instances_1 = instances[\"camera\"]\n",
    "\n",
    "features_1 = torch.stack([instance[\"features\"] for instance in instances_1], dim=0)\n",
    "\n",
    "distance = CosineSimilarity()\n",
    "for instance in instances_0:\n",
    "    features = instance[\"features\"].unsqueeze(0)\n",
    "    # print(features[:, :10])\n",
    "\n",
    "    d = distance(features, features_1)\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_1[:, :10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b09d746b8f7d49e4d7c870418bd04bb5a79717e9e3cc6f92d77e11e9d41b7621"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
