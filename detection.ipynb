{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Leonard\\miniconda3\\envs\\openmmlab\\lib\\site-packages\\mmcv\\__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.patches as patches\n",
    "from datetime import datetime\n",
    "\n",
    "from typing import List\n",
    "\n",
    "import requests\n",
    "from pysolotools.consumers import Solo\n",
    "from pysolotools.converters.solo2coco import SOLO2COCOConverter\n",
    "from pysolotools.core.models import KeypointAnnotationDefinition, RGBCameraCapture\n",
    "from pysolotools.core.models import Frame, RGBCameraCapture\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import torch\n",
    "from mmdet.apis import init_detector, inference_detector, show_result_pyplot\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "# %matplotlib ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 0, 'width': 3000.0, 'height': 3000.0, 'file_name': 'camera_0.png'}\n",
      "{'id': 1, 'width': 3000.0, 'height': 3000.0, 'file_name': 'camera_1.png'}\n"
     ]
    }
   ],
   "source": [
    "class CustomConverter(SOLO2COCOConverter):\n",
    "    \n",
    "    def callback(self, results):\n",
    "        map(super().callback, results)\n",
    "\n",
    "    @staticmethod\n",
    "    def _process_instances(\n",
    "        frame: Frame, idx, output, data_root, solo_kp_map\n",
    "    ) -> Tuple[Dict, List, List, List]:\n",
    "        # logger.info(f\"Processing Frame number: {idx}\")\n",
    "        image_id = idx\n",
    "        sequence_num = frame.sequence\n",
    "        rgb_captures = list(\n",
    "            filter(lambda cap: isinstance(cap, RGBCameraCapture), frame.captures)\n",
    "        )\n",
    "\n",
    "        results = []\n",
    "        for capture_idx, rgb_capture in enumerate(rgb_captures):\n",
    "            img_record = SOLO2COCOConverter._process_rgb_image(\n",
    "                image_id*len(rgb_captures)+capture_idx, rgb_capture, output, data_root, sequence_num\n",
    "                # f'{image_id}_{capture_idx}', rgb_capture, output, data_root, sequence_num\n",
    "            )\n",
    "            (\n",
    "                ann_record,\n",
    "                ins_ann_record,\n",
    "                sem_ann_record,\n",
    "            ) = SOLO2COCOConverter._process_annotations(\n",
    "                image_id*len(rgb_captures)+capture_idx, rgb_capture, sequence_num, data_root, solo_kp_map\n",
    "                # f'{image_id}_{capture_idx}', rgb_capture, sequence_num, data_root, solo_kp_map\n",
    "            )\n",
    "            results.append(\n",
    "                (img_record, ann_record, ins_ann_record, sem_ann_record)\n",
    "            )\n",
    "        return results\n",
    "    \n",
    "    def convert(self, output_path: str, dataset_name: str = \"coco\"):\n",
    "        output = os.path.join(output_path, dataset_name)\n",
    "\n",
    "        solo_kp_map = self._get_solo_kp_map()\n",
    "\n",
    "        for idx, frame in enumerate(self._solo.frames()):\n",
    "            results = self._process_instances(frame, idx, output, self._solo.data_path, solo_kp_map)\n",
    "            self.callback(results)\n",
    "            for result in results:\n",
    "                print(result[0])\n",
    "            break\n",
    "\n",
    "        # for idx, frame in enumerate(self._solo.frames()):\n",
    "        #     self._pool.apply_async(\n",
    "        #         self._process_instances,\n",
    "        #         args=(frame, idx, output, self._solo.data_path, solo_kp_map),\n",
    "        #         callback=self.callback,\n",
    "        #     )\n",
    "        self._pool.close()\n",
    "        self._pool.join()\n",
    "\n",
    "        self._write_out_annotations(output)\n",
    "\n",
    "folder_dir = 'C:/Users/Leonard/AppData/LocalLow/DefaultCompany/Perception2/solo_16'\n",
    "\n",
    "solo = Solo(data_path=folder_dir)\n",
    "dataset = CustomConverter(solo)\n",
    "dataset.convert(output_path='./data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset._images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step0.camera_0.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "folder_dir = 'C:/Users/Leonard/AppData/LocalLow/DefaultCompany/Perception2/solo_16'\n",
    "\n",
    "solo = Solo(data_path=folder_dir)\n",
    "output = os.path.join(solo.data_path, 'coco')\n",
    "# solo_kp_map = self._get_solo_kp_map()\n",
    "\n",
    "for idx, frame in enumerate(solo.frames()):\n",
    "    image_id = idx\n",
    "    sequence_num = frame.sequence\n",
    "    for capture_idx, rgb_capture in enumerate(frame.captures):\n",
    "        img_record = SOLO2COCOConverter._process_rgb_image(\n",
    "            image_id*len(frame.captures)+capture_idx, rgb_capture, output, solo.data_path, sequence_num\n",
    "        )\n",
    "        (\n",
    "            ann_record,\n",
    "            ins_ann_record,\n",
    "            sem_ann_record,\n",
    "        ) = SOLO2COCOConverter._process_annotations(\n",
    "            image_id*len(frame.captures)+capture_idx, rgb_capture, sequence_num, solo.data_path, solo_kp_map\n",
    "        )\n",
    "        # results.append(\n",
    "        #     (img_record, ann_record, ins_ann_record, sem_ann_record)\n",
    "        # )\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def post_process_result(ret):\n",
    "    for u in range(len(ret[1])):\n",
    "        for v in range(len(ret[1][u])):\n",
    "            # remove segmentation result\n",
    "            ret[1][u][v] = np.zeros_like(ret[1][u][v], dtype=bool)\n",
    "            # not remove detection result if it is 'Human' class (idx: 0)\n",
    "            if u == 0: continue\n",
    "            # remove detection result\n",
    "            ret[0][u][v] = np.zeros_like(ret[0][u][v], dtype=np.float32)\n",
    "    return ret\n",
    "\n",
    "for i, frame in enumerate(solo.frames()):\n",
    "    for capture in frame.captures:\n",
    "        # for bbox in capture.annotations:\n",
    "        #     print(bbox.values)\n",
    "        label_dict = {\n",
    "            \"images\": {\n",
    "                \"id\": 1,\n",
    "                \"width\": capture.dimension[0],\n",
    "                \"height\": capture.dimension[1],\n",
    "                \"file_name\": capture.filename\n",
    "            }\n",
    "        }\n",
    "        coco_annotation = COCOeval(annotation_file=coco_annotation_file_path)\n",
    "\n",
    "        # result = inference_detector(obj_det, f'{folder_dir}/sequence.{i}/{capture.filename}')\n",
    "        # result = post_process_result(result)\n",
    "\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python tools/test.py \"./configs/swin/mask_rcnn_swin-t-p4-w7_fpn_fp16_ms-crop-3x_coco.py\" \"./checkpoints/mask_rcnn_swin-t-p4-w7_fpn_fp16_ms-crop-3x_coco_20210908_165006-90a4008c.pth\" --eval bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"categories\": [\n",
    "    {\n",
    "        \"supercategory\": \"default\",\n",
    "        \"id\": 1,\n",
    "        \"name\": \"person\",\n",
    "        \"keypoints\": [],\n",
    "        \"skeleton\": []\n",
    "    },\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b09d746b8f7d49e4d7c870418bd04bb5a79717e9e3cc6f92d77e11e9d41b7621"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
